% Note of ch10 Boosting, Understanding ML.
\section{Boosting}

Boosting is an algorithm that grew out of a theoretical question and became a vary practical
machine learning tool. The boosting approach uses a generalization linear approach to address
two major issues:
\begin{itemize}
	\item Bias-complexity tradeoff.
	\item Computational complexity of learning.The boosting algorithm amplifies the accuracy
		of weak learners.
\end{itemize}

AdaBoost(Adaptive Boost) stemmed from the theoretical question of whether an efficient weak
learner can be "boosted" into an efficient strong learner.

\subsection{WEAK LEARNABILITY}

The fundamental theorem of learning theory characterizes the family of learnable classes and
states that every PAC learnable class can be learned using any ERM algorithm by ignoring
the computational aspect of learning.

\begin{defn}
	($\gamma$-Weak-Learnability).\\
	\begin{itemize}
		\item \emph{$\gamma$-weak-learner}, A : 
			$\exists m_\mathcal{H}:(0,1)\rightarrow\mathbb{N}$,such that,
			$\forall \delta \in (0,1), \forall$ distribution $\mathcal{D}$ over $\mathcal{X}$,
			$\forall f:\mathcal{X}\rightarrow\{ \pm 1 \}$,
			if $m \ge m_{\mathcal{H,D},f}(\delta)$,we have,
			\[
					\mathbb{P}(L_{\mathcal{D},f}(A(S)) \le 1/2-\gamma) \ge 1-\delta
			.\]
		\item $\gamma-weak-learnable$, $\mathcal{H}$ :
			$\exists \gamma-weak-learner$, A for $\mathcal{H}$.
	\end{itemize}
\end{defn}

In chapter6, we have $m_\mathcal{H}(\epsilon, \delta) \ge C_1\frac{d+log(1/\delta)}{\epsilon}$,
so when $d=\infty$ then $\mathcal{H}$ is not $\gamma-weak-learnbale$.
This implies that from the statistical perspective, weak learnbality is also characterized
by the VC dimension of $\mathcal{H}$ and therefore is just as hard as PAC learning.
(Ignoring computational complexity).

Considering computational complexity, we can get efficiently implemented weak learning.
One possible approach is to take a "simple" hypothesis class, denoted B, and to apply
ERM with respect to B as the weak learning algorithm.For this to work, we nned B with 
two properties:
\begin{itemize}
	\item $ERM_B$ is efficiently implementable.
	\item For every sample taht is labeled by some hypothesis from $\mathcal{H}$, any
		$ERM_B$ hypothesis will have an error of at most $1/2-\gamma$.
\end{itemize}

\begin{exam}
	\emph{Weak Learning of 3-Piece Classfiers Using Decision Stumps}
	\begin{itemize}
		\item \emph{3-Piece Classifiers} $\mathcal{H} = \{ h_{\theta_1,\theta_2,b:\theta_1, \theta_2 \in \mathbb{R}},
			\theta_1 < \theta_2, b\in\{ \pm 1 \} \}$\\
			$h_{\theta_1,\theta_2,b}(x) = b \cdot {1} \{ x<\theta_1 \vee x>\theta_2 \}$	
		\item \emph{Decision Stumps} : 
			$B = \{ x \mapsto b \cdot sign(x-\theta) : \theta \in \mathbb{R}, b \in \{ \pm 1 \} \}$
		\item
			\begin{proof}
				($ERM_B$ is a $\gamma-weak-learner$ for $\mathcal{H}$)\\
				Since $\forall \mathcal{D}, \exists h \in B, L_\mathcal{D}(h) \le 1/3$,\\
				In ch6, we have that when $m \ge \log\frac{2/\delta}{\epsilon}$ : 
				\[
					\mathbb{P}\{L_\mathcal{D}(ERM_B(S)) \le min L_\mathcal{D}(h) + \epsilon\} \ge 1-\delta
				.\]
				We set $\epsilon = 1/12$, then we obtain that the error of $ERM_B$ is at most
				$1/3+1/12 = 1/2-1/12$.
			\end{proof}
	\end{itemize}
\end{exam}

\subsubsection{Efficient Implementation of ERM for Decision Stumps}

Let $\mathcal{X} = \mathbb{R}^d$ and consider the base hypothesis class of decision stumps
over $\mathbb{R}^d$, namely,
\[
	\mathcal{H}_{DS} = \{ \mathbf{x} \mapsto sign(\theta - x_i) \cdot b : 
	\theta \in \mathbb{R}, i \in [d], b \in \{ \pm 1 \} \}
.\]

Let $\mathbf{D}$ be a probability vector in $\mathbb{R}^m$ ($\sum_i D_i = 1$ ).
 \[
	 L_{\mathbf{D}}(h) = \sum\limits^m_{i=1} D_i \mathbf{1} \{ h( \mathbf{x}_i \ne y_i ) \}
.\]

ERM:
\begin{equation}
	\label{equ10_1}
	 \underset{j\in[d]}{\min}\ \underset{\theta \in \mathbb{R}} {\min} 
	 \left(
	 \sum\limits_{i:y_i=1} D_i \mathbf{1} \{ x_{i,j} > \theta \}
 + \sum\limits_{i:y_i=-1} D_i \mathbf{1} \{  x_{i,j} \le \theta \}
	 \right)
\end{equation}

Let training set is $x_{1,j} - 1 = x_{0,j} \le x_{1,j} \le x_{2,j} \le \dots \le x_{m,j} \le x_{m+1, j} = x_{m,j} + 1$,
then define $\Theta$: 
\[
	\theta \in 
	\Theta_j = \left\{
		\frac{x_{i,j}+x_{x+1,j}}{2} : i \in x_{\cdot, j}
	\right\}
.\]

We use following equation to calculate Equ(\ref{equ10_1}) in $O(dm)$ instead of $O(dm^2)$.
\[
	F(\theta') = F(\theta) - D_i \mathbf{1} \{ y_i=1 \} + D_i \mathbf{1} \{ y_i = -1 \} 
	= F(\theta) - y_i D_i
.\]

\begin{algorithm}[h!]
	\caption{ERM for Decision Stumps}	
	\begin{algorithmic}
		\Require S = $\{ (\mathbf{x}_1, y_1), \dots, (\mathbf{x}_m, y_m) \}$,
		distribution vector $\mathbf{D}$
		\Ensure $F^* = \infty$ 
		\For {$j = 1, \dots, d$ }
			\State sort S using the j'th coordinate, and denote
			\State $x_{1,j} \le x_{2,j} \le \dots \le x_{m,j} \le x_{m+1,j}
			\overset{def}{=} x_{m,j} + 1$
			\State $F = \sum_{i:y_i=1}D_i$
			\If{$F < F^*$}
				\State $F^* = F, \theta^* = x_{1,j}-1,j^*=j$
			\EndIf
			\For{i=1,\dots,m}
				\State $F = F - y_i D_i$
				\If{ $F < F^*$ and $x_{i,j} \ne x_{i+1,j}$ }
				\State  $F^*=F, \theta^* = (x_{i,j}+x_{i+1,j}), j^* = j$	
				\EndIf
			\EndFor
		\EndFor
		\State\Return $j^*, \theta^*$
	\end{algorithmic}
\end{algorithm}

\subsection{ADABOOST}

AdaBoost constructs $\mathbf{D}^{(t)}$.The weak learner is assumed to return a "weak"
hypothesis, $h_t$, whose error,
\[
	\epsilon_t \overset{def}{=} L_{\mathbf{D}^{(t)}}(h_t) \overset{def}{=}
	\sum\limits^m_{i=1} D^{(t)}_i \mathbf{1} \{ h_t(\mathbf{x}_i) \ne y_i \}
.\]
is at most $\frac{1}{2}-\gamma$.

\begin{algorithm}[h!]
	\caption{AdaBoost}
	\begin{algorithmic}
		\Require S = $\{ (\mathbf{x}_1, y_1), \dots, (\mathbf{x}_m, y_m) \}$,
			weak learner WL, number of rounds T
		
	\end{algorithmic}
\end{algorithm}
